

# Project 2: Continuous Control


## Project details:
For this project, you will work with the Reacher environment.

![title](reacher.gif)
Unity ML-Agents Reacher Environment


In this environment, a double-jointed arm can move to target locations. A reward of +0.1 is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.

The observation space consists of 33 variables corresponding to position, rotation, velocity, and angular velocities of the arm. Each action is a vector with four numbers, corresponding to torque applicable to two joints. Every entry in the action vector should be a number between -1 and 1.

## Getting started:
There are three files needed to train this agent, Continuous_Control.ipynb, ddpg.py and model.py. The trained weights of 'Actor' and 'Critic' are saved in 96_96_actor.pth and 96_96_critic.pth respectively.

The main code, which dictate the interaction between DDPG agent and the environment, is in the Continuous_Control.ipynb. 

ddpg.py defines the agent class

model.py defines the deep neural network models for both 'Actor' and 'Critic' neural networks.  

## Instructions:
1.Download all three files, Continuous_Control.ipynb, ddpg.py and model.py

2.Execute the codes in Continuous_Control.ipynb 



```python

```


```python

```
