{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Continuous Control\n",
    "\n",
    "\n",
    "## Project details:\n",
    "For this project, you will work with the Reacher environment.\n",
    "\n",
    "![title](reacher.gif)\n",
    "Unity ML-Agents Reacher Environment\n",
    "\n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of +0.1 is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of 33 variables corresponding to position, rotation, velocity, and angular velocities of the arm. Each action is a vector with four numbers, corresponding to torque applicable to two joints. Every entry in the action vector.\n",
    "\n",
    "\n",
    "## Learning Algorithm:\n",
    "\n",
    "The agent is based on DDPG algorithm. A deep Neural Network model is used to approximate the state-value function, while the other Neural Network is used to generate the control policy. \n",
    "\n",
    "### The network:\n",
    "#### Actor\n",
    "Inputs: system states (total of 33 states)\n",
    "Outputs: value for each action (total of 4 actions)\n",
    "Hidden Layer 1: fully connected 96 neurons, Relu activation function\n",
    "Hidden Layer 2: fully connected 96 neurons, Relu activation function\n",
    "\n",
    "#### Critic\n",
    "Inputs: system states (total of 33 states)\n",
    "Outputs: state-function value\n",
    "Hidden Layer 1: fully connected 96 neurons, Relu activation function\n",
    "Hidden Layer 2: fully connected 96 neurons, Relu activation function\n",
    "\n",
    "\n",
    "### The DDPG algorithm\n",
    "The DDPG utilizes epsilon-greedy method. The epsilon is selected as 1 at the beginning of the training process to allow more exploration. It delays at a rate of 1e-06 to shift the balance to exploitation as the agent gets better. \n",
    "\n",
    "Experience Replay method was applied to eliminate data correlation and improve training performance. The experience buffer is selected sufficiently large (5e5) to store enough experiences. The DDPG is updated with mini batch of 256 samples, randomly selected from the experience buffer. \n",
    "\n",
    "Soft update technique is applied\n",
    "\n",
    "### Plot of rewards:\n",
    "The following figure shows the score (average of 100 episodes) evolution with training episodes. The score went above 30 at 113th episode while the batch episode average went above 30 at 41st episode:\n",
    "\n",
    "![title](rewards.png)\n",
    "\n",
    "\n",
    "## Future improvement:\n",
    "I am very satisfied with the current DDPG agent's performance. Further investigation will be focused on:\n",
    "\n",
    "1. Visualize the agent in the Unity environment\n",
    "\n",
    "2. Apply other techniques, like PPO and A3C\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
